Training on stratified fold 1/5...

Epoch: 0001/0003 | Batch 0000/1082 | Loss: 0.7900
Epoch: 0001/0003 | Batch 0050/1082 | Loss: 0.0047
Epoch: 0001/0003 | Batch 0100/1082 | Loss: 0.0144
Epoch: 0001/0003 | Batch 0150/1082 | Loss: 0.0008
Epoch: 0001/0003 | Batch 0200/1082 | Loss: 0.0335
Epoch: 0001/0003 | Batch 0250/1082 | Loss: 0.0005
Epoch: 0001/0003 | Batch 0300/1082 | Loss: 0.0008
Epoch: 0001/0003 | Batch 0350/1082 | Loss: 0.0006
Epoch: 0001/0003 | Batch 0400/1082 | Loss: 0.0443
Epoch: 0001/0003 | Batch 0450/1082 | Loss: 0.0053
Epoch: 0001/0003 | Batch 0500/1082 | Loss: 0.0001
Epoch: 0001/0003 | Batch 0550/1082 | Loss: 0.0009
Epoch: 0001/0003 | Batch 0600/1082 | Loss: 0.0071
Epoch: 0001/0003 | Batch 0650/1082 | Loss: 0.0070
Epoch: 0001/0003 | Batch 0700/1082 | Loss: 0.0003
Epoch: 0001/0003 | Batch 0750/1082 | Loss: 0.0230
Epoch: 0001/0003 | Batch 0800/1082 | Loss: 0.0009
Epoch: 0001/0003 | Batch 0850/1082 | Loss: 0.0229
Epoch: 0001/0003 | Batch 0900/1082 | Loss: 0.0001
Epoch: 0001/0003 | Batch 0950/1082 | Loss: 0.0012
Epoch: 0001/0003 | Batch 1000/1082 | Loss: 0.0004
Epoch: 0001/0003 | Batch 1050/1082 | Loss: 0.0011
Epoch: 0001/0003 | Training Loss: 0.0162 | No validation step in this epoch.
Time elapsed: 10.18 min
Epoch: 0002/0003 | Batch 0000/1082 | Loss: 0.0038
Epoch: 0002/0003 | Batch 0050/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0100/1082 | Loss: 0.0005
Epoch: 0002/0003 | Batch 0150/1082 | Loss: 0.0002
Epoch: 0002/0003 | Batch 0200/1082 | Loss: 0.0010
Epoch: 0002/0003 | Batch 0250/1082 | Loss: 0.0003
Epoch: 0002/0003 | Batch 0300/1082 | Loss: 0.0005
Epoch: 0002/0003 | Batch 0350/1082 | Loss: 0.0024
Epoch: 0002/0003 | Batch 0400/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0450/1082 | Loss: 0.0000
Epoch: 0002/0003 | Batch 0500/1082 | Loss: 0.0000
Epoch: 0002/0003 | Batch 0550/1082 | Loss: 0.0057
Epoch: 0002/0003 | Batch 0600/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0650/1082 | Loss: 0.0005
Epoch: 0002/0003 | Batch 0700/1082 | Loss: 0.0000
Epoch: 0002/0003 | Batch 0750/1082 | Loss: 0.0000
Epoch: 0002/0003 | Batch 0800/1082 | Loss: 0.0000
Epoch: 0002/0003 | Batch 0850/1082 | Loss: 0.0005
Epoch: 0002/0003 | Batch 0900/1082 | Loss: 0.0033
Epoch: 0002/0003 | Batch 0950/1082 | Loss: 0.0395
Epoch: 0002/0003 | Batch 1000/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 1050/1082 | Loss: 0.0000
Epoch: 0002/0003 | Training Loss: 0.0021 | No validation step in this epoch.
Time elapsed: 20.37 min
Epoch: 0003/0003 | Batch 0000/1082 | Loss: 0.0003
Epoch: 0003/0003 | Batch 0050/1082 | Loss: 0.0106
Epoch: 0003/0003 | Batch 0100/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0150/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0200/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0250/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0300/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0350/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0400/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0450/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0500/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0550/1082 | Loss: 0.0002
Epoch: 0003/0003 | Batch 0600/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0650/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0700/1082 | Loss: 0.0001
Epoch: 0003/0003 | Batch 0750/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0800/1082 | Loss: 0.0001
Epoch: 0003/0003 | Batch 0850/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0900/1082 | Loss: 0.0003
Epoch: 0003/0003 | Batch 0950/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 1000/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 1050/1082 | Loss: 0.0000
Epoch: 0003/0003 | Training Loss: 0.0022 | No validation step in this epoch.
Time elapsed: 30.59 min
Total Training Time: 30.59 min
Test Accuracy: 99.61850%
Precision: 0.99621
Recall: 0.99618
F1-Score: 0.99618
AUC-ROC: 0.99616
Average Loss: 0.0125
Training on stratified fold 2/5...
Epoch: 0001/0003 | Batch 0000/1082 | Loss: 0.6732
Epoch: 0001/0003 | Batch 0050/1082 | Loss: 0.1037
Epoch: 0001/0003 | Batch 0100/1082 | Loss: 0.0025
Epoch: 0001/0003 | Batch 0150/1082 | Loss: 0.0432
Epoch: 0001/0003 | Batch 0200/1082 | Loss: 0.0016
Epoch: 0001/0003 | Batch 0250/1082 | Loss: 0.0017
Epoch: 0001/0003 | Batch 0300/1082 | Loss: 0.0007
Epoch: 0001/0003 | Batch 0350/1082 | Loss: 0.0015
Epoch: 0001/0003 | Batch 0400/1082 | Loss: 0.0009
Epoch: 0001/0003 | Batch 0450/1082 | Loss: 0.0010
Epoch: 0001/0003 | Batch 0500/1082 | Loss: 0.0038
Epoch: 0001/0003 | Batch 0550/1082 | Loss: 0.0025
Epoch: 0001/0003 | Batch 0600/1082 | Loss: 0.0141
Epoch: 0001/0003 | Batch 0650/1082 | Loss: 0.0005
Epoch: 0001/0003 | Batch 0700/1082 | Loss: 0.0098
Epoch: 0001/0003 | Batch 0750/1082 | Loss: 0.0002
Epoch: 0001/0003 | Batch 0800/1082 | Loss: 0.0004
Epoch: 0001/0003 | Batch 0850/1082 | Loss: 0.0004
Epoch: 0001/0003 | Batch 0900/1082 | Loss: 0.0001
Epoch: 0001/0003 | Batch 0950/1082 | Loss: 0.0003
Epoch: 0001/0003 | Batch 1000/1082 | Loss: 0.0011
Epoch: 0001/0003 | Batch 1050/1082 | Loss: 0.0006
Epoch: 0001/0003 | Training Loss: 0.0129 | No validation step in this epoch.
Time elapsed: 10.23 min
Epoch: 0002/0003 | Batch 0000/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0050/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0100/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0150/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0200/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0250/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0300/1082 | Loss: 0.0000
Epoch: 0002/0003 | Batch 0350/1082 | Loss: 0.0000
Epoch: 0002/0003 | Batch 0400/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0450/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0500/1082 | Loss: 0.0014
Epoch: 0002/0003 | Batch 0550/1082 | Loss: 0.0004
Epoch: 0002/0003 | Batch 0600/1082 | Loss: 0.0075
Epoch: 0002/0003 | Batch 0650/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0700/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0750/1082 | Loss: 0.0003
Epoch: 0002/0003 | Batch 0800/1082 | Loss: 0.0011
Epoch: 0002/0003 | Batch 0850/1082 | Loss: 0.0002
Epoch: 0002/0003 | Batch 0900/1082 | Loss: 0.0012
Epoch: 0002/0003 | Batch 0950/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 1000/1082 | Loss: 0.0000
Epoch: 0002/0003 | Batch 1050/1082 | Loss: 0.0000
Epoch: 0002/0003 | Training Loss: 0.0032 | No validation step in this epoch.
Time elapsed: 20.45 min
Epoch: 0003/0003 | Batch 0000/1082 | Loss: 0.0024
Epoch: 0003/0003 | Batch 0050/1082 | Loss: 0.0001
Epoch: 0003/0003 | Batch 0100/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0150/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0200/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0250/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0300/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0350/1082 | Loss: 0.0002
Epoch: 0003/0003 | Batch 0400/1082 | Loss: 0.0019
Epoch: 0003/0003 | Batch 0450/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0500/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0550/1082 | Loss: 0.0003
Epoch: 0003/0003 | Batch 0600/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0650/1082 | Loss: 0.0080
Epoch: 0003/0003 | Batch 0700/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0750/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0800/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0850/1082 | Loss: 0.0001
Epoch: 0003/0003 | Batch 0900/1082 | Loss: 0.0003
Epoch: 0003/0003 | Batch 0950/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 1000/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 1050/1082 | Loss: 0.0000
Epoch: 0003/0003 | Training Loss: 0.0017 | No validation step in this epoch.
Time elapsed: 30.63 min
Total Training Time: 30.63 min
Test Accuracy: 99.93064%
Precision: 0.99931
Recall: 0.99931
F1-Score: 0.99931
AUC-ROC: 0.99931
Average Loss: 0.0025
Training on stratified fold 3/5...
Epoch: 0001/0003 | Batch 0000/1082 | Loss: 0.7719
Epoch: 0001/0003 | Batch 0050/1082 | Loss: 0.0071
Epoch: 0001/0003 | Batch 0100/1082 | Loss: 0.0023
Epoch: 0001/0003 | Batch 0150/1082 | Loss: 0.0029
Epoch: 0001/0003 | Batch 0200/1082 | Loss: 0.0050
Epoch: 0001/0003 | Batch 0250/1082 | Loss: 0.0662
Epoch: 0001/0003 | Batch 0300/1082 | Loss: 0.0275
Epoch: 0001/0003 | Batch 0350/1082 | Loss: 0.0018
Epoch: 0001/0003 | Batch 0400/1082 | Loss: 0.0004
Epoch: 0001/0003 | Batch 0450/1082 | Loss: 0.0375
Epoch: 0001/0003 | Batch 0500/1082 | Loss: 0.0041
Epoch: 0001/0003 | Batch 0550/1082 | Loss: 0.0001
Epoch: 0001/0003 | Batch 0600/1082 | Loss: 0.1144
Epoch: 0001/0003 | Batch 0650/1082 | Loss: 0.0001
Epoch: 0001/0003 | Batch 0700/1082 | Loss: 0.0006
Epoch: 0001/0003 | Batch 0750/1082 | Loss: 0.0010
Epoch: 0001/0003 | Batch 0800/1082 | Loss: 0.0098
Epoch: 0001/0003 | Batch 0850/1082 | Loss: 0.0001
Epoch: 0001/0003 | Batch 0900/1082 | Loss: 0.0001
Epoch: 0001/0003 | Batch 0950/1082 | Loss: 0.0009
Epoch: 0001/0003 | Batch 1000/1082 | Loss: 0.0001
Epoch: 0001/0003 | Batch 1050/1082 | Loss: 0.0013
Epoch: 0001/0003 | Training Loss: 0.0150 | No validation step in this epoch.
Time elapsed: 10.18 min
Epoch: 0002/0003 | Batch 0000/1082 | Loss: 0.0004
Epoch: 0002/0003 | Batch 0050/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0100/1082 | Loss: 0.0008
Epoch: 0002/0003 | Batch 0150/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0200/1082 | Loss: 0.0003
Epoch: 0002/0003 | Batch 0250/1082 | Loss: 0.0000
Epoch: 0002/0003 | Batch 0300/1082 | Loss: 0.0000
Epoch: 0002/0003 | Batch 0350/1082 | Loss: 0.0000
Epoch: 0002/0003 | Batch 0400/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0450/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0500/1082 | Loss: 0.0009
Epoch: 0002/0003 | Batch 0550/1082 | Loss: 0.0002
Epoch: 0002/0003 | Batch 0600/1082 | Loss: 0.0002
Epoch: 0002/0003 | Batch 0650/1082 | Loss: 0.0046
Epoch: 0002/0003 | Batch 0700/1082 | Loss: 0.0000
Epoch: 0002/0003 | Batch 0750/1082 | Loss: 0.0000
Epoch: 0002/0003 | Batch 0800/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0850/1082 | Loss: 0.0013
Epoch: 0002/0003 | Batch 0900/1082 | Loss: 0.0000
Epoch: 0002/0003 | Batch 0950/1082 | Loss: 0.0000
Epoch: 0002/0003 | Batch 1000/1082 | Loss: 0.0000
Epoch: 0002/0003 | Batch 1050/1082 | Loss: 0.0009
Epoch: 0002/0003 | Training Loss: 0.0020 | No validation step in this epoch.
Time elapsed: 20.36 min
Epoch: 0003/0003 | Batch 0000/1082 | Loss: 0.0004
Epoch: 0003/0003 | Batch 0050/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0100/1082 | Loss: 0.0098
Epoch: 0003/0003 | Batch 0150/1082 | Loss: 0.0013
Epoch: 0003/0003 | Batch 0200/1082 | Loss: 0.0001
Epoch: 0003/0003 | Batch 0250/1082 | Loss: 0.0361
Epoch: 0003/0003 | Batch 0300/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0350/1082 | Loss: 0.0003
Epoch: 0003/0003 | Batch 0400/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0450/1082 | Loss: 0.0001
Epoch: 0003/0003 | Batch 0500/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0550/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0600/1082 | Loss: 0.0002
Epoch: 0003/0003 | Batch 0650/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0700/1082 | Loss: 0.0001
Epoch: 0003/0003 | Batch 0750/1082 | Loss: 0.0001
Epoch: 0003/0003 | Batch 0800/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0850/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0900/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0950/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 1000/1082 | Loss: 0.0043
Epoch: 0003/0003 | Batch 1050/1082 | Loss: 0.0004
Epoch: 0003/0003 | Training Loss: 0.0029 | No validation step in this epoch.
Time elapsed: 30.54 min
Total Training Time: 30.54 min
Test Accuracy: 99.78035%
Precision: 0.99781
Recall: 0.99780
F1-Score: 0.99780
AUC-ROC: 0.99781
Average Loss: 0.0066
Training on stratified fold 4/5...

Epoch: 0001/0003 | Batch 0000/1082 | Loss: 0.7160
Epoch: 0001/0003 | Batch 0050/1082 | Loss: 0.0650
Epoch: 0001/0003 | Batch 0100/1082 | Loss: 0.0096
Epoch: 0001/0003 | Batch 0150/1082 | Loss: 0.0034
Epoch: 0001/0003 | Batch 0200/1082 | Loss: 0.0008
Epoch: 0001/0003 | Batch 0250/1082 | Loss: 0.0570
Epoch: 0001/0003 | Batch 0300/1082 | Loss: 0.0026
Epoch: 0001/0003 | Batch 0350/1082 | Loss: 0.0003
Epoch: 0001/0003 | Batch 0400/1082 | Loss: 0.0004
Epoch: 0001/0003 | Batch 0450/1082 | Loss: 0.0055
Epoch: 0001/0003 | Batch 0500/1082 | Loss: 0.0009
Epoch: 0001/0003 | Batch 0550/1082 | Loss: 0.0073
Epoch: 0001/0003 | Batch 0600/1082 | Loss: 0.0261
Epoch: 0001/0003 | Batch 0650/1082 | Loss: 0.0025
Epoch: 0001/0003 | Batch 0700/1082 | Loss: 0.0001
Epoch: 0001/0003 | Batch 0750/1082 | Loss: 0.0002
Epoch: 0001/0003 | Batch 0800/1082 | Loss: 0.0004
Epoch: 0001/0003 | Batch 0850/1082 | Loss: 0.0012
Epoch: 0001/0003 | Batch 0900/1082 | Loss: 0.0663
Epoch: 0001/0003 | Batch 0950/1082 | Loss: 0.0005
Epoch: 0001/0003 | Batch 1000/1082 | Loss: 0.0005
Epoch: 0001/0003 | Batch 1050/1082 | Loss: 0.0014
Epoch: 0001/0003 | Training Loss: 0.0142 | No validation step in this epoch.
Time elapsed: 10.20 min
Epoch: 0002/0003 | Batch 0000/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0050/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0100/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0150/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0200/1082 | Loss: 0.0000
Epoch: 0002/0003 | Batch 0250/1082 | Loss: 0.0000
Epoch: 0002/0003 | Batch 0300/1082 | Loss: 0.0007
Epoch: 0002/0003 | Batch 0350/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0400/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0450/1082 | Loss: 0.0028
Epoch: 0002/0003 | Batch 0500/1082 | Loss: 0.0000
Epoch: 0002/0003 | Batch 0550/1082 | Loss: 0.0000
Epoch: 0002/0003 | Batch 0600/1082 | Loss: 0.0019
Epoch: 0002/0003 | Batch 0650/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0700/1082 | Loss: 0.0002
Epoch: 0002/0003 | Batch 0750/1082 | Loss: 0.0004
Epoch: 0002/0003 | Batch 0800/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0850/1082 | Loss: 0.0000
Epoch: 0002/0003 | Batch 0900/1082 | Loss: 0.0254
Epoch: 0002/0003 | Batch 0950/1082 | Loss: 0.0000
Epoch: 0002/0003 | Batch 1000/1082 | Loss: 0.0015
Epoch: 0002/0003 | Batch 1050/1082 | Loss: 0.0001
Epoch: 0002/0003 | Training Loss: 0.0028 | No validation step in this epoch.
Time elapsed: 20.40 min
Epoch: 0003/0003 | Batch 0000/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0050/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0100/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0150/1082 | Loss: 0.0003
Epoch: 0003/0003 | Batch 0200/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0250/1082 | Loss: 0.0001
Epoch: 0003/0003 | Batch 0300/1082 | Loss: 0.0001
Epoch: 0003/0003 | Batch 0350/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0400/1082 | Loss: 0.0055
Epoch: 0003/0003 | Batch 0450/1082 | Loss: 0.0002
Epoch: 0003/0003 | Batch 0500/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0550/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0600/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0650/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0700/1082 | Loss: 0.0026
Epoch: 0003/0003 | Batch 0750/1082 | Loss: 0.0004
Epoch: 0003/0003 | Batch 0800/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0850/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0900/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0950/1082 | Loss: 0.0003
Epoch: 0003/0003 | Batch 1000/1082 | Loss: 0.0006
Epoch: 0003/0003 | Batch 1050/1082 | Loss: 0.0001
Epoch: 0003/0003 | Training Loss: 0.0028 | No validation step in this epoch.
Time elapsed: 30.60 min
Total Training Time: 30.60 min
Test Accuracy: 99.83815%
Precision: 0.99838
Recall: 0.99838
F1-Score: 0.99838
AUC-ROC: 0.99838
Average Loss: 0.0064
Training on stratified fold 5/5...

Epoch: 0001/0003 | Batch 0000/1082 | Loss: 0.7006
Epoch: 0001/0003 | Batch 0050/1082 | Loss: 0.0955
Epoch: 0001/0003 | Batch 0100/1082 | Loss: 0.0275
Epoch: 0001/0003 | Batch 0150/1082 | Loss: 0.0015
Epoch: 0001/0003 | Batch 0200/1082 | Loss: 0.1482
Epoch: 0001/0003 | Batch 0250/1082 | Loss: 0.0037
Epoch: 0001/0003 | Batch 0300/1082 | Loss: 0.0019
Epoch: 0001/0003 | Batch 0350/1082 | Loss: 0.0005
Epoch: 0001/0003 | Batch 0400/1082 | Loss: 0.0003
Epoch: 0001/0003 | Batch 0450/1082 | Loss: 0.0002
Epoch: 0001/0003 | Batch 0500/1082 | Loss: 0.0002
Epoch: 0001/0003 | Batch 0550/1082 | Loss: 0.0088
Epoch: 0001/0003 | Batch 0600/1082 | Loss: 0.0001
Epoch: 0001/0003 | Batch 0650/1082 | Loss: 0.0002
Epoch: 0001/0003 | Batch 0700/1082 | Loss: 0.0002
Epoch: 0001/0003 | Batch 0750/1082 | Loss: 0.0045
Epoch: 0001/0003 | Batch 0800/1082 | Loss: 0.0001
Epoch: 0001/0003 | Batch 0850/1082 | Loss: 0.0001
Epoch: 0001/0003 | Batch 0900/1082 | Loss: 0.0677
Epoch: 0001/0003 | Batch 0950/1082 | Loss: 0.0017
Epoch: 0001/0003 | Batch 1000/1082 | Loss: 0.0001
Epoch: 0001/0003 | Batch 1050/1082 | Loss: 0.0004
Epoch: 0001/0003 | Training Loss: 0.0145 | No validation step in this epoch.
Time elapsed: 10.17 min
Epoch: 0002/0003 | Batch 0000/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0050/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0100/1082 | Loss: 0.0002
Epoch: 0002/0003 | Batch 0150/1082 | Loss: 0.0000
Epoch: 0002/0003 | Batch 0200/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0250/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0300/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0350/1082 | Loss: 0.0000
Epoch: 0002/0003 | Batch 0400/1082 | Loss: 0.0002
Epoch: 0002/0003 | Batch 0450/1082 | Loss: 0.0061
Epoch: 0002/0003 | Batch 0500/1082 | Loss: 0.0004
Epoch: 0002/0003 | Batch 0550/1082 | Loss: 0.0000
Epoch: 0002/0003 | Batch 0600/1082 | Loss: 0.0010
Epoch: 0002/0003 | Batch 0650/1082 | Loss: 0.0001
Epoch: 0002/0003 | Batch 0700/1082 | Loss: 0.0000
Epoch: 0002/0003 | Batch 0750/1082 | Loss: 0.0000
Epoch: 0002/0003 | Batch 0800/1082 | Loss: 0.0000
Epoch: 0002/0003 | Batch 0850/1082 | Loss: 0.0000
Epoch: 0002/0003 | Batch 0900/1082 | Loss: 0.0016
Epoch: 0002/0003 | Batch 0950/1082 | Loss: 0.0000
Epoch: 0002/0003 | Batch 1000/1082 | Loss: 0.0000
Epoch: 0002/0003 | Batch 1050/1082 | Loss: 0.0041
Epoch: 0002/0003 | Training Loss: 0.0021 | No validation step in this epoch.
Time elapsed: 20.35 min
Epoch: 0003/0003 | Batch 0000/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0050/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0100/1082 | Loss: 0.0011
Epoch: 0003/0003 | Batch 0150/1082 | Loss: 0.0019
Epoch: 0003/0003 | Batch 0200/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0250/1082 | Loss: 0.0005
Epoch: 0003/0003 | Batch 0300/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0350/1082 | Loss: 0.0023
Epoch: 0003/0003 | Batch 0400/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0450/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0500/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0550/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0600/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 0650/1082 | Loss: 0.0001
Epoch: 0003/0003 | Batch 0700/1082 | Loss: 0.0001
Epoch: 0003/0003 | Batch 0750/1082 | Loss: 0.0002
Epoch: 0003/0003 | Batch 0800/1082 | Loss: 0.0013
Epoch: 0003/0003 | Batch 0850/1082 | Loss: 0.0001
Epoch: 0003/0003 | Batch 0900/1082 | Loss: 0.0377
Epoch: 0003/0003 | Batch 0950/1082 | Loss: 0.0000
Epoch: 0003/0003 | Batch 1000/1082 | Loss: 0.0012
Epoch: 0003/0003 | Batch 1050/1082 | Loss: 0.0028
Epoch: 0003/0003 | Training Loss: 0.0025 | No validation step in this epoch.
Time elapsed: 30.52 min
Total Training Time: 30.52 min
Test Accuracy: 99.91329%
Precision: 0.99913
Recall: 0.99913
F1-Score: 0.99913
AUC-ROC: 0.99913
Average Loss: 0.0029
Stratified Cross-validation completed. Results: [{'accuracy': 0.9961849710982659, 'precision': np.float64(0.9962119752279702), 'recall': np.float64(0.9961849710982659), 'f1_score': np.float64(0.9961848372944564), 'auc_roc': np.float64(0.9961634589681903), 'loss': 0.012470514801100181}, {'accuracy': 0.9993063583815028, 'precision': np.float64(0.9993063841482301), 'recall': np.float64(0.9993063583815028), 'f1_score': np.float64(0.9993063579086912), 'auc_roc': np.float64(0.9993056669339748), 'loss': 0.002487907463242711}, {'accuracy': 0.9978034682080925, 'precision': np.float64(0.9978073356710915), 'recall': np.float64(0.9978034682080925), 'f1_score': np.float64(0.9978034815949456), 'auc_roc': np.float64(0.9978114140604115), 'loss': 0.006593132523452086}, {'accuracy': 0.9983815028901734, 'precision': np.float64(0.9983836412844731), 'recall': np.float64(0.9983815028901734), 'f1_score': np.float64(0.9983814912087875), 'auc_roc': np.float64(0.9983753118939654), 'loss': 0.006443301615623347}, {'accuracy': 0.9991328978553674, 'precision': np.float64(0.9991330617272157), 'recall': np.float64(0.9991328978553674), 'f1_score': np.float64(0.9991328963196175), 'auc_roc': np.float64(0.9991311807173515), 'loss': 0.00287226927009057}]